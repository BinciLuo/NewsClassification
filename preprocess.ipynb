{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THU\\体育.txt\n",
      "THU\\娱乐.txt\n",
      "THU\\家居.txt\n",
      "THU\\彩票.txt\n",
      "THU\\房产.txt\n",
      "THU\\教育.txt\n",
      "THU\\时尚.txt\n",
      "THU\\时政.txt\n",
      "THU\\星座.txt\n",
      "THU\\游戏.txt\n",
      "THU\\社会.txt\n",
      "THU\\科技.txt\n",
      "THU\\股票.txt\n",
      "THU\\财经.txt\n",
      "836075 836075\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 遍历data目录下的所有txt文件\n",
    "    for category in os.listdir(data_dir):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        print(category_path)\n",
    "        if category_path.endswith(\".txt\"):\n",
    "            with open(category_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                lines = [line.split('\\t')[1].strip() for line in lines]\n",
    "                data += lines\n",
    "                labels += [category[:-4]] * len(lines)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def split_train_test_data(data, labels, test_size=0.2, random_state=42):\n",
    "    # 划分训练集和测试集\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data, labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "# 使用示例\n",
    "data_dir = \"THU\"  # 数据目录路径\n",
    "data, labels = load_data(data_dir)\n",
    "print(len(data), len(labels))\n",
    "train_data, test_data, train_labels, test_labels = split_train_test_data(data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['女子3年借给老赖150万 讨债无果欲跳楼\\n',\n",
       " '台阳改建成的小舞台(图)\\n',\n",
       " '保值相机马上停产 理光R10低价仅售1850\\n',\n",
       " '《掌上明珠》累坏陶大宇 暴瘦10斤\\n',\n",
       " '快讯：动视暴雪第一季度净利润5美元同比增32%\\n',\n",
       " '带笔记本开越野车 龙门寺住持读EMBA(组图)\\n',\n",
       " '儿童血铅从何来 小心幼儿园室内环境污染\\n',\n",
       " '王全安：我跟汪小菲是各取所爱皆大欢喜\\n',\n",
       " '未雨绸缪：高中生留学资金和语言是重点\\n']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump('trd', train_data)\n",
    "pickle_dump('trl', train_labels)\n",
    "pickle_dump('ted', test_data)\n",
    "pickle_dump('tel', test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [list(line) for line in train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668860"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(sentences, vector_size=64, window=5, min_count=1, sg=0)\n",
    "word2vec_model.wv['<UNK>'] = np.zeros((64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key), word2vec_model.vector_size))\n",
    "for i, word in enumerate(word2vec_model.wv.index_to_key):\n",
    "    embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5128, 64)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, word2vec_model, max_length):\n",
    "    words = list(text)  # 将文本拆分为单词\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model.wv:\n",
    "            vector.append(word2vec_model.wv[word])  # 如果单词在Word2Vec模型中存在，将其词向量添加到序列中\n",
    "        else:\n",
    "            vector.append(word2vec_model.wv['<UNK>'])  # 否则，使用<UNK>标记的向量代替未登录词\n",
    "    # 填充或截断向量以适应固定长度\n",
    "    if len(vector) < max_length:\n",
    "        vector.extend([word2vec_model.wv['<UNK>']] * (max_length - len(vector)))  # 填充向量\n",
    "    else:\n",
    "        vector = vector[:max_length]  # 截断向量以达到最大长度\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, word2vec_model, max_length):\n",
    "    words = list(text)  # 将文本拆分为单词\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model.wv:\n",
    "            vector.append(word2vec_model.wv.key_to_index[word])  # 如果单词在Word2Vec模型中存在，将其词向量添加到序列中\n",
    "        else:\n",
    "            vector.append(word2vec_model.wv.key_to_index['<UNK>'])  # 否则，使用<UNK>标记的向量代替未登录词\n",
    "    # 填充或截断向量以适应固定长度\n",
    "    if len(vector) < max_length:\n",
    "        vector.extend([word2vec_model.wv.key_to_index['<UNK>']] * (max_length - len(vector)))  # 填充向量\n",
    "    else:\n",
    "        vector = vector[:max_length]  # 截断向量以达到最大长度\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelset = list(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = dict(enumerate(labelset))\n",
    "label2idx = dict([(v, k) for k, v in enumerate(labelset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([text_to_id(text, word2vec_model, 30) for text in train_data])\n",
    "y_train = np.array([label2idx[label] for label in train_labels])\n",
    "\n",
    "X_test = np.array([text_to_id(text, word2vec_model, 30) for text in test_data])\n",
    "y_test = np.array([label2idx[label] for label in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump('wv', word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_classes):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        avg_pool = torch.mean(lstm_out, dim=1)\n",
    "        output = self.fc(avg_pool)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "num_classes = len(label2idx)\n",
    "model = BiLSTMClassifier(embedding_matrix, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.LongTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5128, 64)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "X_train_tensor = X_train_tensor.to('cuda')\n",
    "y_train_tensor = y_train_tensor.to('cuda')\n",
    "X_test_tensor = X_test_tensor.to('cuda')\n",
    "y_test_tensor = y_test_tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 100\n",
      " batch 200\n",
      " batch 300\n",
      " batch 400\n",
      " batch 500\n",
      " batch 600\n",
      " batch 700\n",
      " batch 800\n",
      " batch 900\n",
      " batch 1000\n",
      " batch 1100\n",
      " batch 1200\n",
      " batch 1300\n",
      " batch 1400\n",
      " batch 1500\n",
      " batch 1600\n",
      " batch 1700\n",
      " batch 1800\n",
      " batch 1900\n",
      " batch 2000\n",
      " batch 2100\n",
      " batch 2200\n",
      " batch 2300\n",
      " batch 2400\n",
      " batch 2500\n",
      " batch 2600\n",
      " batch 2700\n",
      " batch 2800\n",
      " batch 2900\n",
      " batch 3000\n",
      " batch 3100\n",
      " batch 3200\n",
      " batch 3300\n",
      " batch 3400\n",
      " batch 3500\n",
      " batch 3600\n",
      " batch 3700\n",
      " batch 3800\n",
      " batch 3900\n",
      " batch 4000\n",
      " batch 4100\n",
      " batch 4200\n",
      " batch 4300\n",
      " batch 4400\n",
      " batch 4500\n",
      " batch 4600\n",
      " batch 4700\n",
      " batch 4800\n",
      " batch 4900\n",
      " batch 5000\n",
      " batch 5100\n",
      " batch 5200\n",
      "Epoch [1/5] Loss: 0.5673\n",
      " batch 100\n",
      " batch 200\n",
      " batch 300\n",
      " batch 400\n",
      " batch 500\n",
      " batch 600\n",
      " batch 700\n",
      " batch 800\n",
      " batch 900\n",
      " batch 1000\n",
      " batch 1100\n",
      " batch 1200\n",
      " batch 1300\n",
      " batch 1400\n",
      " batch 1500\n",
      " batch 1600\n",
      " batch 1700\n",
      " batch 1800\n",
      " batch 1900\n",
      " batch 2000\n",
      " batch 2100\n",
      " batch 2200\n",
      " batch 2300\n",
      " batch 2400\n",
      " batch 2500\n",
      " batch 2600\n",
      " batch 2700\n",
      " batch 2800\n",
      " batch 2900\n",
      " batch 3000\n",
      " batch 3100\n",
      " batch 3200\n",
      " batch 3300\n",
      " batch 3400\n",
      " batch 3500\n",
      " batch 3600\n",
      " batch 3700\n",
      " batch 3800\n",
      " batch 3900\n",
      " batch 4000\n",
      " batch 4100\n",
      " batch 4200\n",
      " batch 4300\n",
      " batch 4400\n",
      " batch 4500\n",
      " batch 4600\n",
      " batch 4700\n",
      " batch 4800\n",
      " batch 4900\n",
      " batch 5000\n",
      " batch 5100\n",
      " batch 5200\n",
      "Epoch [2/5] Loss: 0.5413\n",
      " batch 100\n",
      " batch 200\n",
      " batch 300\n",
      " batch 400\n",
      " batch 500\n",
      " batch 600\n",
      " batch 700\n",
      " batch 800\n",
      " batch 900\n",
      " batch 1000\n",
      " batch 1100\n",
      " batch 1200\n",
      " batch 1300\n",
      " batch 1400\n",
      " batch 1500\n",
      " batch 1600\n",
      " batch 1700\n",
      " batch 1800\n",
      " batch 1900\n",
      " batch 2000\n",
      " batch 2100\n",
      " batch 2200\n",
      " batch 2300\n",
      " batch 2400\n",
      " batch 2500\n",
      " batch 2600\n",
      " batch 2700\n",
      " batch 2800\n",
      " batch 2900\n",
      " batch 3000\n",
      " batch 3100\n",
      " batch 3200\n",
      " batch 3300\n",
      " batch 3400\n",
      " batch 3500\n",
      " batch 3600\n",
      " batch 3700\n",
      " batch 3800\n",
      " batch 3900\n",
      " batch 4000\n",
      " batch 4100\n",
      " batch 4200\n",
      " batch 4300\n",
      " batch 4400\n",
      " batch 4500\n",
      " batch 4600\n",
      " batch 4700\n",
      " batch 4800\n",
      " batch 4900\n",
      " batch 5000\n",
      " batch 5100\n",
      " batch 5200\n",
      "Epoch [3/5] Loss: 0.5226\n",
      " batch 100\n",
      " batch 200\n",
      " batch 300\n",
      " batch 400\n",
      " batch 500\n",
      " batch 600\n",
      " batch 700\n",
      " batch 800\n",
      " batch 900\n",
      " batch 1000\n",
      " batch 1100\n",
      " batch 1200\n",
      " batch 1300\n",
      " batch 1400\n",
      " batch 1500\n",
      " batch 1600\n",
      " batch 1700\n",
      " batch 1800\n",
      " batch 1900\n",
      " batch 2000\n",
      " batch 2100\n",
      " batch 2200\n",
      " batch 2300\n",
      " batch 2400\n",
      " batch 2500\n",
      " batch 2600\n",
      " batch 2700\n",
      " batch 2800\n",
      " batch 2900\n",
      " batch 3000\n",
      " batch 3100\n",
      " batch 3200\n",
      " batch 3300\n",
      " batch 3400\n",
      " batch 3500\n",
      " batch 3600\n",
      " batch 3700\n",
      " batch 3800\n",
      " batch 3900\n",
      " batch 4000\n",
      " batch 4100\n",
      " batch 4200\n",
      " batch 4300\n",
      " batch 4400\n",
      " batch 4500\n",
      " batch 4600\n",
      " batch 4700\n",
      " batch 4800\n",
      " batch 4900\n",
      " batch 5000\n",
      " batch 5100\n",
      " batch 5200\n",
      "Epoch [4/5] Loss: 0.5084\n",
      " batch 100\n",
      " batch 200\n",
      " batch 300\n",
      " batch 400\n",
      " batch 500\n",
      " batch 600\n",
      " batch 700\n",
      " batch 800\n",
      " batch 900\n",
      " batch 1000\n",
      " batch 1100\n",
      " batch 1200\n",
      " batch 1300\n",
      " batch 1400\n",
      " batch 1500\n",
      " batch 1600\n",
      " batch 1700\n",
      " batch 1800\n",
      " batch 1900\n",
      " batch 2000\n",
      " batch 2100\n",
      " batch 2200\n",
      " batch 2300\n",
      " batch 2400\n",
      " batch 2500\n",
      " batch 2600\n",
      " batch 2700\n",
      " batch 2800\n",
      " batch 2900\n",
      " batch 3000\n",
      " batch 3100\n",
      " batch 3200\n",
      " batch 3300\n",
      " batch 3400\n",
      " batch 3500\n",
      " batch 3600\n",
      " batch 3700\n",
      " batch 3800\n",
      " batch 3900\n",
      " batch 4000\n",
      " batch 4100\n",
      " batch 4200\n",
      " batch 4300\n",
      " batch 4400\n",
      " batch 4500\n",
      " batch 4600\n",
      " batch 4700\n",
      " batch 4800\n",
      " batch 4900\n",
      " batch 5000\n",
      " batch 5100\n",
      " batch 5200\n",
      "Epoch [5/5] Loss: 0.4970\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 5\n",
    "model.to('cuda')\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        i+=1\n",
    "        if(i % 100 == 0):\n",
    "            print('\\r batch {}'.format(i))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 2.00 GiB total capacity; 393.71 MiB already allocated; 0 bytes free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 4\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m      5\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[0;32m      6\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\gtjsl\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[103], line 10\u001b[0m, in \u001b[0;36mBiLSTMClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m---> 10\u001b[0m     lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedded)\n\u001b[0;32m     11\u001b[0m     avg_pool \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(lstm_out, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(avg_pool)\n",
      "File \u001b[1;32mc:\\Users\\gtjsl\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\gtjsl\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 2.00 GiB total capacity; 393.71 MiB already allocated; 0 bytes free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {loss.item()}')\n",
    "    torch.save(model, os.path.join(prefix, f'{epoch}_.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.40%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor[:1000])\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    predicted = predicted.to('cpu')\n",
    "    accuracy = accuracy_score(y_test[:1000], predicted.numpy()[:1000])\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['戴尔提升收购3PAR价格至每股24.30美元', '美股周一低收中国概念股多数下跌', '黄加李泡世界杯：网络原创成人礼', '国足开创主力从未合练先河', '谁是你榜样(组图)', '全国第一家非公企业党建展览馆', '爱情测试：爱情占你生命中的比重(图)', '8或2012年上市', '汇丰晋信大盘股票基金即将发行', '用于心理援助']\n"
     ]
    }
   ],
   "source": [
    "print(test_data[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['股票', '股票', '科技', '体育', '家居', '时政', '星座', '科技', '财经', '股票']\n",
      "['科技', '科技', '科技', '体育', '时尚', '家居', '星座', '科技', '财经', '娱乐']\n"
     ]
    }
   ],
   "source": [
    "y_pred = predicted.numpy()\n",
    "label_pred = [idx2label[idx] for idx in y_pred]\n",
    "print(label_pred[10:20])\n",
    "print(test_labels[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor[1:2].shape\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor[1:2])\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "predicted = predicted.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'家居'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = text_to_id(\"讨债无果欲跳楼\", word2vec_model, 50)\n",
    "t = torch.LongTensor([t]).to('cuda')\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(t)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "predicted = predicted.to('cpu').numpy()\n",
    "idx2label[predicted[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
